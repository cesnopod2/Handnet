{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "022b88ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc2a4e5",
   "metadata": {},
   "source": [
    "## Custom dataset generator\n",
    "### it requires path to the custom dataset direcotry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "066428c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Custom_dataset:\n",
    "    def __init__(self, path):\n",
    "        self.gestures = glob.glob(path+ \"\\*\")\n",
    "        self.number_classes = len(os.listdir(path))\n",
    "        \n",
    "    def get_items(self): \n",
    "        labels_list = []\n",
    "        gestures_list = []\n",
    "        \n",
    "        for gesture_id, gesture in enumerate (self.gestures):\n",
    "            people = glob.glob(gesture+ \"\\*\")\n",
    "            for person in people: \n",
    "                essais = glob.glob(person + \"\\*\")\n",
    "                for essai in essais :\n",
    "                    labels_list.append(gesture_id)\n",
    "                    data = np.load(essai)\n",
    "                    data = Custom_dataset.preprocess_data(data)\n",
    "                    gestures_list.append(data)\n",
    "        labels = tf.keras.utils.to_categorical(labels_list, num_classes=self.number_classes).astype(int)\n",
    "        return np.array(gestures_list), labels\n",
    "        \n",
    "    @staticmethod\n",
    "    def preprocess_data(data):\n",
    "\n",
    "        blank_frame = np.zeros_like(data[0,:])\n",
    "        data_len = data.shape[0]\n",
    "        if data_len > 30:\n",
    "            data = data[:30,:]  \n",
    "        else:\n",
    "            x = (30 - data_len)\n",
    "            padding = x // 2\n",
    "            adding = 0\n",
    "            for i in range(0, padding):\n",
    "                data = np.insert(data, 0, blank_frame, axis=0)\n",
    "                data = np.insert(data, -1, blank_frame, axis=0)\n",
    "            if x % 2 == 1:\n",
    "                data = np.insert(data, -1, blank_frame, axis=0)\n",
    "            \n",
    "        return data \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a46b77d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# DHG 14/18 dataset generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa4ad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DHG_Hand_dataset():\n",
    "    def __init__(self, processing_type = \"clean_data\"):\n",
    "        self.data = None\n",
    "        self.save_path = r\"C:\\HandNET\\data\"\n",
    "        self.data_path = r\"C:\\HandNET\\data\\LSTM_data_point_position_features.npy\"\n",
    "        self.info_df = pd.read_pickle(r\"C:\\HandNET\\data\\paths_start_stop_label.pkl\")\n",
    "        self.labels = tf.keras.utils.to_categorical(self.info_df[\"label\"].tolist(), num_classes=14).astype(int)\n",
    "        self.processing_type = processing_type\n",
    "        self.filename = \"LSTM_data_\"+self.processing_type+\".npy\"\n",
    "        self.data_path = os.path.join(\"C:\\HandNET\\data\\LSTM_data\",self.filename)\n",
    "\n",
    "    def get_item(self):\n",
    "        if not os.path.exists(self.data_path):\n",
    "            print(\"file does not exist\")\n",
    "            self.data = self.process()\n",
    "        else:\n",
    "            print(\"file exists, data has been loaded\")\n",
    "            self.data = np.load(self.data_path)\n",
    "        return self.data, self.labels\n",
    "\n",
    "    def process(self):\n",
    "\n",
    "        data = []\n",
    "        for index in range(0, self.info_df.shape[0]):\n",
    "    \n",
    "            data.append(self.create_gesture_dataframe(self.info_df[\"path\"][index],self.info_df[\"start_frame\"][index],self.info_df[\"stop_frame\"][index]))\n",
    "        print(data[0].shape)\n",
    "        print(\"saving data ...\")\n",
    "        final_data = np.array(data)\n",
    "        np.save(self.data_path, final_data)\n",
    "        print(\"data saved.\")\n",
    "        return final_data\n",
    "\n",
    "    def create_gesture_dataframe(self, path,start_id,stop_id):\n",
    "        lines = []\n",
    "        test_gesture = []\n",
    "        file = open(path, \"r\")\n",
    "        for pos, l_num in enumerate(file):\n",
    "\n",
    "            if pos >=start_id and pos<=stop_id:\n",
    "                lines.append((l_num))\n",
    "\n",
    "        for line in lines:\n",
    "            x = line.split()\n",
    "            x = list(map(float, x))\n",
    "            gesture = np.array(x)\n",
    "            gesture = Hand_dataset_2.change_data_order(gesture)\n",
    "            test_gesture.append(gesture)\n",
    "\n",
    "        return Hand_dataset_2.preprocess_data(test_gesture)\n",
    "\n",
    "    @staticmethod\n",
    "    def change_data_order(data):\n",
    "        output = np.zeros((63,))\n",
    "        for i in range(0,63,21): \n",
    "\n",
    "            output[0+i] = data[0+i]\n",
    "            output[1+i] = data[18+i]\n",
    "            output[2+i] = data[19+i]\n",
    "            output[3+i] = data[20+i]\n",
    "            output[4+i] = data[21+i]\n",
    "            output[5+i] =  data[14+i]\n",
    "            output[6+i] = data[15+i]\n",
    "            output[7+i]  = data[16+i]\n",
    "            output[8+i] =data[17+i]\n",
    "            output[9+i] = data[10+i]\n",
    "            output[10+i] = data[11+i]\n",
    "            output[11+i] = data[12+i]\n",
    "            output[12+i] = data[13+i]\n",
    "            output[13+i] = data[6+i]\n",
    "            output[14+i] =data[7+i]\n",
    "            output[15+i] = data[8+i]\n",
    "            output[16+i] = data[9+i]\n",
    "            output[17+i] = data[1+i]\n",
    "            output[18+i] = data[2+i]\n",
    "            output[19+i] = data[3+i]\n",
    "            output[20+i] = data[4+i]\n",
    "            \n",
    "        return output\n",
    "    @staticmethod \n",
    "    def normalize_data(data):\n",
    "            \n",
    "        processed_data = data/100\n",
    "        return processed_data \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def wrist_reference(data):\n",
    "        wrist_data_x = data[0]\n",
    "        wrist_data_y = data[1]\n",
    "        wrist_data_z = data[2]\n",
    "\n",
    "        for i in range(0,63,3):\n",
    "            data[i] = data[i] - wrist_data_x\n",
    "            data[i+1] = data[i+1] - wrist_data_y\n",
    "            data[i+2] = data[i+2] - wrist_data_z\n",
    "\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess_data(data):\n",
    "        blank_frame = np.zeros_like(data[0])\n",
    "        data_len = len(data)\n",
    "        if data_len > 30:\n",
    "            data = data[:30]  # check if not 49 index\n",
    "        else:\n",
    "            x = (30 - data_len)\n",
    "            padding = x // 2\n",
    "            adding = 0\n",
    "            for i in range(0, padding):\n",
    "                data.insert(0, blank_frame)\n",
    "                data.insert(-1, blank_frame)\n",
    "\n",
    "            if x % 2 == 1:\n",
    "                data.insert(-1, blank_frame)\n",
    "            else:\n",
    "                pass\n",
    "        return np.array(data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d1bf8f",
   "metadata": {},
   "source": [
    "### To initialize DHG dataset generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b70574",
   "metadata": {},
   "outputs": [],
   "source": [
    "gestures, labels = DHG_Hand_dataset(\"clean_data_mediapipe_3\").get_item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c360ea17",
   "metadata": {},
   "source": [
    "### To initialize custom dataset generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cc4b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "gestures, labels = Custom_dataset(r\"C:\\Users\\Filip\\Desktop\\Final_custom_gesture\").get_items()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd03b0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(gestures, labels, test_size=0.2, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddf0f55",
   "metadata": {},
   "source": [
    "### LSTM  model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3431c074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense,Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedda46f",
   "metadata": {},
   "source": [
    "### Trail 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d9d2606",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_trail_1 \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m()\n\u001b[0;32m      2\u001b[0m model_trail_1\u001b[38;5;241m.\u001b[39madd(LSTM(\u001b[38;5;241m64\u001b[39m, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,return_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m63\u001b[39m)))\n\u001b[0;32m      3\u001b[0m model_trail_1\u001b[38;5;241m.\u001b[39madd(Flatten())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "model_trail_1 = Sequential()\n",
    "model_trail_1.add(LSTM(64, return_sequences=True,return_state = False, input_shape=(30,63)))\n",
    "model_trail_1.add(Flatten())\n",
    "model_trail_1.add(Dense(64, activation='relu'))\n",
    "model_trail_1.add(Dense(9, activation='softmax'))\n",
    "model_trail_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39deaa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_epochs = 78\n",
    "model_trail_1.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "history_feature = model_trail_1.fit(x_train, y_train, epochs=_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984e5aa4",
   "metadata": {},
   "source": [
    "### Trail 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab67f527",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trail_2 = Sequential()\n",
    "model_trail_2.add(LSTM(64, return_sequences=True,return_state = False, input_shape=(30,63)))\n",
    "model_trail_2.add(LSTM(64, return_sequences=True))\n",
    "model_trail_2.add(Flatten())\n",
    "model_trail_2.add(Dense(64, activation='relu'))\n",
    "model_trail_2.add(Dense(9, activation='softmax'))\n",
    "model_trail_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee33b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "_epochs = 78\n",
    "model_trail_2.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "history_feature = model_trail_2.fit(x_train, y_train, epochs=_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba6af8e",
   "metadata": {},
   "source": [
    "### Trail 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68616775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 30, 64)            32768     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 30, 128)           98816     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3840)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               491648    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 585       \n",
      "=================================================================\n",
      "Total params: 632,073\n",
      "Trainable params: 632,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_trail_3 = Sequential()\n",
    "model_trail_3.add(LSTM(64, return_sequences=True,return_state = False, input_shape=(30,63)))\n",
    "model_trail_3.add(LSTM(128, return_sequences=True))\n",
    "model_trail_3.add(Flatten())\n",
    "model_trail_3.add(Dense(128, activation='relu'))\n",
    "model_trail_3.add(Dense(64, activation='relu'))\n",
    "model_trail_3.add(Dense(9, activation='softmax'))\n",
    "model_trail_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96815683",
   "metadata": {},
   "outputs": [],
   "source": [
    "_epochs = 78\n",
    "model_trail_3.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "history_feature = model_trail_3.fit(x_train, y_train, epochs=_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d69abf0",
   "metadata": {},
   "source": [
    "### Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffbb0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True,return_state = False, input_shape=(30,63)))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(9, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4207c3ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_epochs = 78\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "history_feature = model.fit(x_train, y_train, epochs=_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f998c5d",
   "metadata": {},
   "source": [
    "### Model training visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fa9614",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_train = history_feature.history['loss']\n",
    "loss_val = history_feature.history['categorical_accuracy']\n",
    "epochs = range(1,_epochs+1)\n",
    "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "# plt.plot(epochs, loss_val, 'b', label='Categorical accuracy')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47148a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_val = history_feature.history['categorical_accuracy']\n",
    "epochs = range(1,_epochs+1)\n",
    "\n",
    "plt.plot(epochs, loss_val, 'b', label='Categorical accuracy')\n",
    "plt.title('Categorical accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fbbd81",
   "metadata": {},
   "source": [
    "### Model saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d085d76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trail_2.save('C:/HandNET/model/model_trail_2.h5')\n",
    "# model.load_weights(r\"C:\\HandNET\\model\\action_acc_85.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec476c82",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab2cf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "y_preds = model_trail_2.predict(x_test)\n",
    "print(y_preds)\n",
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "y_preds = np.argmax(y_preds, axis=1).tolist()\n",
    "# print(f\"preds : {y_preds}\")\n",
    "# print(f\"true : {ytrue}\")\n",
    "# print(\"confusion matrix\")\n",
    "\n",
    "print(multilabel_confusion_matrix(ytrue, y_preds))\n",
    "cfs_matrix = multilabel_confusion_matrix(ytrue, y_preds)\n",
    "\n",
    "print(f\"accuracy is :{accuracy_score(ytrue, y_preds)}\")\n",
    "\n",
    "print(f\"precision :{precision_score(ytrue, y_preds, average='macro')}\")\n",
    "print(f\"recall : {recall_score(ytrue, y_preds, average='macro')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
